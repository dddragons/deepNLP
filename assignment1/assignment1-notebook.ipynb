{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and DeepLearning Assignment1\n",
    "### Working progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import sklearn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the softmax function\n",
    "#### q1_softmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute the softmax function for each row of the input x.\n",
    "\n",
    "    It is crucial that this function is optimized for speed because\n",
    "    it will be used frequently in later code. You might find numpy\n",
    "    functions np.exp, np.sum, np.reshape, np.max, and numpy\n",
    "    broadcasting useful for this task.\n",
    "\n",
    "    Numpy broadcasting documentation:\n",
    "    http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\n",
    "\n",
    "    You should also make sure that your code works for a single\n",
    "    N-dimensional vector (treat the vector as a single row) and\n",
    "    for M x N matrices. This may be useful for testing later. Also,\n",
    "    make sure that the dimensions of the output match the input.\n",
    "\n",
    "    You must implement the optimization in problem 1(a) of the\n",
    "    written assignment!\n",
    "\n",
    "    Arguments:\n",
    "    x -- A N dimensional vector or M x N dimensional numpy matrix.\n",
    "\n",
    "    Return:\n",
    "    x -- You are allowed to modify x in-place\n",
    "    \"\"\"\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    if len(x.shape) > 1:\n",
    "        # Matrix\n",
    "        ### YOUR CODE HERE\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        maxs = np.max(x, axis=1) # An array of max values of each line\n",
    "        x = x - maxs.reshape(maxs.shape[0], 1) # for each line, subtract the max value\n",
    "        sums = np.sum(np.exp(x), axis = 1)\n",
    "        x = np.exp(x) / sums.reshape(sums.shape[0], 1)\n",
    "        ### END YOUR CODE\n",
    "    else:\n",
    "        # Vector\n",
    "        ### YOUR CODE HERE\n",
    "        # raise NotImplementedError\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "        ### END YOUR CODE\n",
    "\n",
    "    assert x.shape == orig_shape\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the numpy array broadcasting operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-22.73105858, -22.26894142,   0.        , -22.        ],\n",
       "       [-11.73105858, -11.26894142,   0.        , -10.        ],\n",
       "       [ -3.        ,  -2.        ,  -1.        ,   0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0.26894142, 0.73105858, 23, 1],\n",
    "    [0.26894142, 0.73105858, 12, 2],\n",
    "    [2, 3, 4, 5]])\n",
    "maxs = np.max(x, axis=1) # An array of max values of each line\n",
    "x = x - maxs.reshape(maxs.shape[0], 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.34284749e-10,   2.13167810e-10,   9.99999999e-01,\n",
       "          2.78946809e-10],\n",
       "       [  8.03965182e-06,   1.27623948e-05,   9.99933801e-01,\n",
       "          4.53969243e-05],\n",
       "       [  3.20586033e-02,   8.71443187e-02,   2.36882818e-01,\n",
       "          6.43914260e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums = np.sum(np.exp(x), axis = 1)\n",
    "x = np.exp(x) / sums.reshape(sums.shape[0], 1)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    ans1 = np.array([0.26894142,  0.73105858])\n",
    "    assert np.allclose(test1, ans1, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    ans2 = np.array([\n",
    "        [0.26894142, 0.73105858],\n",
    "        [0.26894142, 0.73105858]])\n",
    "    assert np.allclose(test2, ans2, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    ans3 = np.array([0.73105858, 0.26894142])\n",
    "    assert np.allclose(test3, ans3, rtol=1e-05, atol=1e-06)\n",
    "\n",
    "    print \"You should be able to verify these results by hand!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "You should be able to verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the sigmoid and its gradient\n",
    "#### q2_sigmoid.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    s = 1/ (1 + np.exp(-x))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def sigmoid_grad(s):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input s should be the sigmoid\n",
    "    function value of your original input x.\n",
    "\n",
    "    Arguments:\n",
    "    s -- A scalar or numpy array.\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    ds = s * (1 - s)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.73105858,  0.88079708],\n",
       "       [ 0.26894142,  0.11920292]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[1, 2], [-1, -2]])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print f\n",
    "    f_ans = np.array([\n",
    "        [0.73105858, 0.88079708],\n",
    "        [0.26894142, 0.11920292]])\n",
    "    assert np.allclose(f, f_ans, rtol=1e-05, atol=1e-06)\n",
    "    print g\n",
    "    g_ans = np.array([\n",
    "        [0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])\n",
    "    assert np.allclose(g, g_ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"You should verify these results by hand!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n",
      "You should verify these results by hand!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sigmoid_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a gradient checker\n",
    "#### q2_gradcheck.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First implement a gradient checker by filling in the following functions\n",
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" Gradient check for a function f.\n",
    "\n",
    "    Arguments:\n",
    "    f -- a function that takes a single argument and outputs the\n",
    "         cost and its gradients\n",
    "    x -- the point (numpy array) to check the gradient at\n",
    "    \"\"\"\n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)\n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4        # Do not change this!\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    count = 0\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        # Try modifying x[ix] with h defined above to compute\n",
    "        # numerical gradients. Make sure you call random.setstate(rndstate)\n",
    "        # before calling f(x) each time. This will make it possible\n",
    "        # to test cost functions with built in randomness later.\n",
    "\n",
    "        ### YOUR CODE HERE:\n",
    "        #raise NotImplementedError\n",
    "        random.setstate(rndstate)\n",
    "        current_value = x[ix] \n",
    "        ### calculate outcomes when the input at ix point change -h\n",
    "        x[ix] = current_value - h\n",
    "        y1, _ = f(x)\n",
    "        random.setstate(rndstate)\n",
    "        ### calculate outcomes when the input at ix point change h\n",
    "        x[ix] = current_value + h\n",
    "        y2, _ = f(x)\n",
    "        numgrad = (y2 - y1) / (2*h)\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (\n",
    "                grad[ix], numgrad)\n",
    "            count = count + 1\n",
    "\n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check found: \" + str(count) + \" erors!\"\n",
    "\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check found: 0 erors!\n",
      "Gradient check found: 0 erors!\n",
      "Gradient check found: 0 erors!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement one-layer NN forward and backward propagation\n",
    "#### q2_neural.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation for a two-layer sigmoidal network\n",
    "\n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "\n",
    "    Arguments:\n",
    "    data -- M x Dx matrix, where each row is a training example.\n",
    "    labels -- M x Dy matrix, where each row is a one-hot vector.\n",
    "    params -- Model parameters, these are unpacked for you.\n",
    "    dimensions -- A tuple of input dimension, number of hidden units\n",
    "                  and output dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    N = dimensions[0]\n",
    "    \n",
    "    z1 = data.dot(W1) + b1\n",
    "    h = sigmoid(z1)\n",
    "    z2 = h.dot(W2) + b2\n",
    "    y = softmax(z2)\n",
    "    cost = (np.sum(-labels * np.log(y))) / N\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    delta1 = (y - labels) / N\n",
    "    gradb2 = np.sum(delta1, axis = 0)\n",
    "    gradW2_ind = delta1[:, np.newaxis, :] * h[:,:,np.newaxis]\n",
    "    gradW2 = np.sum(gradW2_ind, axis = 0)\n",
    "    deriv_h = sigmoid_grad(h)\n",
    "    delta2 = np.dot(delta1, W2.T)\n",
    "    gradb1_ind = delta2 * deriv_h\n",
    "    gradb1 = np.sum(gradb1_ind, axis = 0)\n",
    "    #gradW1 = data.T.dot(delta2 * deriv_h)\n",
    "    gradW1_ind = gradb1_ind[:, np.newaxis, :] * data[:, :, np.newaxis]\n",
    "    gradW1 = np.sum(gradW1_ind, axis = 0)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(),\n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "\n",
    "    return cost, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check failed.\n",
      "First gradient error found at index (11,)\n",
      "Your gradient: -0.053841 \t Numerical gradient: -0.053851\n",
      "Gradient check failed.\n",
      "First gradient error found at index (24,)\n",
      "Your gradient: 0.054640 \t Numerical gradient: 0.054652\n",
      "Gradient check failed.\n",
      "First gradient error found at index (49,)\n",
      "Your gradient: 0.039215 \t Numerical gradient: 0.039203\n",
      "Gradient check failed.\n",
      "First gradient error found at index (61,)\n",
      "Your gradient: 0.033504 \t Numerical gradient: 0.033515\n",
      "Gradient check failed.\n",
      "First gradient error found at index (62,)\n",
      "Your gradient: 0.033225 \t Numerical gradient: 0.033206\n",
      "Gradient check failed.\n",
      "First gradient error found at index (63,)\n",
      "Your gradient: -0.175517 \t Numerical gradient: -0.175534\n",
      "Gradient check failed.\n",
      "First gradient error found at index (75,)\n",
      "Your gradient: -0.019533 \t Numerical gradient: -0.019548\n",
      "Gradient check failed.\n",
      "First gradient error found at index (77,)\n",
      "Your gradient: 0.086491 \t Numerical gradient: 0.086501\n",
      "Gradient check failed.\n",
      "First gradient error found at index (82,)\n",
      "Your gradient: 0.116661 \t Numerical gradient: 0.116649\n",
      "Gradient check failed.\n",
      "First gradient error found at index (92,)\n",
      "Your gradient: 0.070666 \t Numerical gradient: 0.070642\n",
      "Gradient check failed.\n",
      "First gradient error found at index (95,)\n",
      "Your gradient: -0.016404 \t Numerical gradient: -0.016418\n",
      "Gradient check failed.\n",
      "First gradient error found at index (97,)\n",
      "Your gradient: 0.021299 \t Numerical gradient: 0.021311\n",
      "Gradient check failed.\n",
      "First gradient error found at index (103,)\n",
      "Your gradient: -0.228817 \t Numerical gradient: -0.228829\n",
      "Gradient check failed.\n",
      "First gradient error found at index (112,)\n",
      "Your gradient: 0.162673 \t Numerical gradient: 0.162638\n",
      "Gradient check found: 14 erors!\n"
     ]
    }
   ],
   "source": [
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using\n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i, random.randint(0,dimensions[2]-1)] = 1\n",
    "\n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params:\n",
    "        forward_backward_prop(data, labels, params, dimensions), params)\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step implementation and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20\n",
    "dimensions = [10, 5, 10]\n",
    "data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "labels = np.zeros((N, dimensions[2]))\n",
    "ofs = 0\n",
    "Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "ofs += Dx * H\n",
    "b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "ofs += H\n",
    "W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "ofs += H * Dy\n",
    "b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "for i in xrange(N):\n",
    "    labels[i, random.randint(0,dimensions[2]-1)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.20112769097463"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = data.dot(W1) + b1\n",
    "h = sigmoid(z1)\n",
    "z2 = h.dot(W2) + b2\n",
    "y = softmax(z2)\n",
    "cost = np.sum(-labels * np.log(y))\n",
    "cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function\n",
    "\n",
    "    Implement a function that normalizes each row of a matrix to have\n",
    "    unit length.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    norm = np.sqrt(np.sum(x*x, axis = 1))\n",
    "    x = x / norm.reshape(norm.shape[0], 1)\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[2,3,4], [4,5,6]])\n",
    "y = normalizeRows(x)\n",
    "np.sum(y[0] *y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]]))\n",
    "    print x\n",
    "    ans = np.array([[0.6,0.8],[0.4472136,0.89442719]])\n",
    "    assert np.allclose(x, ans, rtol=1e-05, atol=1e-06)\n",
    "    print \"\"\n",
    "\n",
    "test_normalize_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, assuming the softmax prediction function and cross\n",
    "    entropy loss.\n",
    "\n",
    "    Arguments:\n",
    "    predicted -- numpy ndarray, predicted word vector (\\hat{v} in\n",
    "                 the written component)\n",
    "    target -- integer, the index of the target word\n",
    "    outputVectors -- \"output\" vectors (as rows) for all tokens\n",
    "    dataset -- needed for negative sampling, unused here.\n",
    "\n",
    "    Return:\n",
    "    cost -- cross entropy cost for the softmax word prediction\n",
    "    gradPred -- the gradient with respect to the predicted word\n",
    "           vector\n",
    "    grad -- the gradient with respect to all the other word\n",
    "           vectors (should be: the gradient with respect to the output vectors)\n",
    "\n",
    "    We will not provide starter code for this function, but feel\n",
    "    free to reference the code you previously wrote for this\n",
    "    assignment!\n",
    "    \"\"\"\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    ## Forward cost calculation\n",
    "    \n",
    "    #\"\"\"\n",
    "    \n",
    "    #print \"my code: softmaxCostAndGradient\"\n",
    "    \n",
    "    y = np.zeros(outputVectors.shape[0])\n",
    "    y[target] = 1\n",
    "    y_hat = softmax(np.dot(predicted, outputVectors.T))\n",
    "    cost = np.sum(-y * np.log(y_hat))\n",
    "    \n",
    "    ## Gradient calculation\n",
    "    y_diff = y_hat - y\n",
    "    gradPred = np.dot(y_diff, outputVectors)\n",
    "    grad = np.outer(y_diff, predicted)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print \"reference code: softmaxCostAndGradient\"\n",
    "\n",
    "    affine = np.dot(outputVectors, predicted)\n",
    "    softmaxOutput = softmax(affine)[:, np.newaxis]\n",
    "    cost  = -np.log(softmaxOutput[target])\n",
    "\n",
    "    gradPred = -outputVectors[target] +  \\\n",
    "    np.sum(outputVectors*softmaxOutput, axis=0).transpose()\n",
    "    grad = np.dot(softmaxOutput, predicted[:, np.newaxis].transpose())\n",
    "    grad[target,:] -= predicted\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNegativeSamples(target, dataset, K):\n",
    "    \"\"\" Samples K indexes which are not the target \"\"\"\n",
    "\n",
    "    indices = [None] * K\n",
    "    for k in xrange(K):\n",
    "        newidx = dataset.sampleTokenIdx()\n",
    "        while newidx == target:\n",
    "            newidx = dataset.sampleTokenIdx()\n",
    "        indices[k] = newidx\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset,\n",
    "                               K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models\n",
    "\n",
    "    Implement the cost and gradients for one predicted word vector\n",
    "    and one target word vector as a building block for word2vec\n",
    "    models, using the negative sampling technique. K is the sample\n",
    "    size.\n",
    "\n",
    "    Note: See test_word2vec below for dataset's initialization.\n",
    "\n",
    "    Arguments/Return Specifications: same as softmaxCostAndGradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Sampling of indices is done for you. Do not modify this if you\n",
    "    # wish to match the autograder and receive points!\n",
    "    indices = [target]\n",
    "    indices.extend(getNegativeSamples(target, dataset, K))\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    ## Forward cost calculation\n",
    "    \n",
    "    #\"\"\"\n",
    "    \n",
    "    #print \"my code: negSamplingCostAndGradient\"\n",
    "    \n",
    "    # get the K+1 values from target and negative samples\n",
    "    y1 = predicted.dot(outputVectors[indices, :].T)\n",
    "    signs = -1 * np.ones(len(indices))\n",
    "    signs[0] = 1\n",
    "    y2 = sigmoid(y1 * signs)\n",
    "    cost = -1 * np.sum(np.log(y2))\n",
    "    \n",
    "    ## Gradient calculation\n",
    "    y3 = (y2 - 1) * signs\n",
    "    gradPred = np.sum(y3[:, np.newaxis] * outputVectors[indices, :], axis = 0)\n",
    "    \n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    #grad[target] = (sigmoid(predicted.dot(outputVectors[target])) - 1) * predicted\n",
    "    \n",
    "    otherGrads = np.outer(y3, predicted)\n",
    "    for i in range(len(indices)):\n",
    "        grad[indices[i]] += otherGrads[i]\n",
    "    \n",
    "    \"\"\"\n",
    "    print \"reference code: negSamplingCostAndGradient\"\n",
    "    \n",
    "    negativeSamples = [dataset.sampleTokenIdx() for i in range(K)]\n",
    "    sigmoidTargetPred = sigmoid(outputVectors[target,:].transpose().dot(predicted))\n",
    "    cost = -np.log(sigmoidTargetPred)   \n",
    "    gradPred = (sigmoidTargetPred - 1.0)*outputVectors[target,:]\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    grad[target,:] = predicted * (sigmoidTargetPred - 1.0)\n",
    "\n",
    "    for sample in negativeSamples:\n",
    "        sigmoidSamplePredicted = \\\n",
    "        sigmoid(-outputVectors[sample,:].transpose().dot(predicted))\n",
    "        cost -= np.log(sigmoidSamplePredicted)\n",
    "        gradPred += (1.0 - sigmoidSamplePredicted)*outputVectors[sample,:]\n",
    "        grad[sample,:] += (1.0 - sigmoidSamplePredicted)*predicted.transpose()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testCostAndGradient(target, predictedAndOutputVectors, dataset,\n",
    "                       costAndGradient=softmaxCostAndGradient):\n",
    "    \n",
    "    allGrad = np.zeros(predictedAndOutputVectors.shape)\n",
    "    \n",
    "    predicted = predictedAndOutputVectors[0]\n",
    "    outputVectors = predictedAndOutputVectors[1:]\n",
    "    \n",
    "    cost, gradPred, grad = costAndGradient(predicted, target, outputVectors,\n",
    "                                          dataset)\n",
    "    allGrad[0] = gradPred\n",
    "    allGrad[1:] = grad\n",
    "    \n",
    "    return cost, allGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = type('dummy', (), {})()\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0,4)], \\\n",
    "        [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36597665,  0.18481607, -0.91208778],\n",
       "       [ 0.64868529, -0.5850643 , -0.4867311 ],\n",
       "       [ 0.16416583, -0.0599129 ,  0.98461161],\n",
       "       [-0.01344871,  0.80812007, -0.58886423],\n",
       "       [ 0.9874962 , -0.04726513,  0.15039038],\n",
       "       [ 0.47581947,  0.80895638,  0.34523239]])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors = normalizeRows(np.random.randn(6,3))\n",
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for costAndGradient ====\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 1)\n",
      "Your gradient: 0.165732 \t Numerical gradient: 0.165701\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 0)\n",
      "Your gradient: -0.767939 \t Numerical gradient: -0.767714\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 1)\n",
      "Your gradient: 0.387805 \t Numerical gradient: 0.388000\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 2)\n",
      "Your gradient: -1.913860 \t Numerical gradient: -1.913596\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 0)\n",
      "Your gradient: -0.301973 \t Numerical gradient: -0.301915\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 1)\n",
      "Your gradient: 0.152495 \t Numerical gradient: 0.152585\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 2)\n",
      "Your gradient: -0.752579 \t Numerical gradient: -0.752546\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 0)\n",
      "Your gradient: -0.487680 \t Numerical gradient: -0.487550\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 1)\n",
      "Your gradient: 0.246276 \t Numerical gradient: 0.246407\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 2)\n",
      "Your gradient: -1.215397 \t Numerical gradient: -1.215265\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 0)\n",
      "Your gradient: 0.228433 \t Numerical gradient: 0.228362\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 1)\n",
      "Your gradient: -0.115357 \t Numerical gradient: -0.115417\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 2)\n",
      "Your gradient: 0.569302 \t Numerical gradient: 0.569220\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 0)\n",
      "Your gradient: -0.152220 \t Numerical gradient: -0.152193\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 1)\n",
      "Your gradient: 0.076870 \t Numerical gradient: 0.076917\n",
      "Gradient check found: 15 erors!\n"
     ]
    }
   ],
   "source": [
    "print \"==== Gradient check for costAndGradient ====\"\n",
    "gradcheck_naive(lambda vec: testCostAndGradient(3, \n",
    "                                                vec,\n",
    "                                                dataset, \n",
    "                                                negSamplingCostAndGradient), \n",
    "                dummy_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "             dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec\n",
    "\n",
    "    Implement the skip-gram model in this function.\n",
    "\n",
    "    Arguments:\n",
    "    currrentWord -- a string of the current center word\n",
    "    C -- integer, context size\n",
    "    contextWords -- list of no more than 2*C strings, the context words\n",
    "    tokens -- a dictionary that maps words to their indices in\n",
    "              the word vector list\n",
    "    inputVectors -- \"input\" word vectors (as rows) for all tokens\n",
    "    outputVectors -- \"output\" word vectors (as rows) for all tokens\n",
    "    word2vecCostAndGradient -- the cost and gradient function for\n",
    "                               a prediction vector given the target\n",
    "                               word vectors, could be one of the two\n",
    "                               cost functions you implemented above.\n",
    "\n",
    "    Return:\n",
    "    cost -- the cost function value for the skip-gram model\n",
    "    grad -- the gradient with respect to the word vectors\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    \n",
    "    predicted_ind = tokens[currentWord]\n",
    "    predicted = inputVectors[predicted_ind]\n",
    "    \n",
    "    for w in contextWords:\n",
    "        target_ind = tokens[w]\n",
    "        #target = outputVectors[target_ind]\n",
    "        cost_target, grad_pred, grad_U = \\\n",
    "        word2vecCostAndGradient(predicted, target_ind, outputVectors, dataset)\n",
    "        cost += cost_target\n",
    "        gradIn[predicted_ind] += grad_pred\n",
    "        gradOut += grad_U\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors,\n",
    "         dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\"CBOW model in word2vec\n",
    "\n",
    "    Implement the continuous bag-of-words model in this function.\n",
    "\n",
    "    Arguments/Return specifications: same as the skip-gram model\n",
    "\n",
    "    Extra credit: Implementing CBOW is optional, but the gradient\n",
    "    derivations are not. If you decide not to implement CBOW, remove\n",
    "    the NotImplementedError.\n",
    "    \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    #raise NotImplementedError\n",
    "    target_ind = tokens[currentWord]\n",
    "    target = outputVectors[target_ind]\n",
    "    \n",
    "    context_indices = []\n",
    "    v_hat = np.zeros(inputVectors.shape[1])\n",
    "    for w in contextWords:\n",
    "        context_ind = tokens[w]\n",
    "        context_indices.append(context_ind)\n",
    "        v_hat += inputVectors[context_ind]\n",
    "        \n",
    "    cost_target, grad_pred, grad_U = \\\n",
    "    word2vecCostAndGradient(v_hat, target_ind, outputVectors, dataset)\n",
    "    cost += cost_target\n",
    "    gradOut += grad_U\n",
    "    for i in range(len(context_indices)):\n",
    "        gradIn[context_indices[i]] += grad_pred\n",
    "        \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return cost, gradIn, gradOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C,\n",
    "                         word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "\n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "\n",
    "        c, gin, gout = word2vecModel(\n",
    "            centerword, C1, context, tokens, inputVectors, outputVectors,\n",
    "            dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "\n",
    "    return cost, grad\n",
    "\n",
    "\n",
    "def test_word2vec():\n",
    "    \"\"\" Interface to the dataset for negative sampling \"\"\"\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], \\\n",
    "            [tokens[random.randint(0,4)] for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, softmaxCostAndGradient),\n",
    "        dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(\n",
    "        cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient),\n",
    "        dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"],\n",
    "        dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset,\n",
    "        negSamplingCostAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n",
      "==== Gradient check for skip-gram ====\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 2)\n",
      "Your gradient: 0.148685 \t Numerical gradient: 0.148663\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 1)\n",
      "Your gradient: -0.109647 \t Numerical gradient: -0.109635\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 2)\n",
      "Your gradient: -0.099611 \t Numerical gradient: -0.099634\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 2)\n",
      "Your gradient: 0.529433 \t Numerical gradient: 0.529403\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 2)\n",
      "Your gradient: -0.026513 \t Numerical gradient: -0.026534\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 2)\n",
      "Your gradient: 0.052118 \t Numerical gradient: 0.052090\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 1)\n",
      "Your gradient: 0.180330 \t Numerical gradient: 0.180361\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 2)\n",
      "Your gradient: 0.467211 \t Numerical gradient: 0.467269\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 0)\n",
      "Your gradient: 0.265767 \t Numerical gradient: 0.265712\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 1)\n",
      "Your gradient: 0.125358 \t Numerical gradient: 0.125297\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 2)\n",
      "Your gradient: -0.140985 \t Numerical gradient: -0.141040\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 1)\n",
      "Your gradient: 0.015873 \t Numerical gradient: 0.015850\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 2)\n",
      "Your gradient: 0.013609 \t Numerical gradient: 0.013595\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 0)\n",
      "Your gradient: 0.142201 \t Numerical gradient: 0.142191\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 1)\n",
      "Your gradient: -0.273350 \t Numerical gradient: -0.273375\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 2)\n",
      "Your gradient: -0.385745 \t Numerical gradient: -0.385774\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 0)\n",
      "Your gradient: 0.004020 \t Numerical gradient: 0.004032\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 1)\n",
      "Your gradient: -0.048212 \t Numerical gradient: -0.048227\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 2)\n",
      "Your gradient: 0.045909 \t Numerical gradient: 0.045863\n",
      "Gradient check found: 19 erors!\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 1)\n",
      "Your gradient: -0.741513 \t Numerical gradient: -0.741483\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 2)\n",
      "Your gradient: -0.410775 \t Numerical gradient: -0.410828\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 1)\n",
      "Your gradient: -0.756728 \t Numerical gradient: -0.756692\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 2)\n",
      "Your gradient: -0.781484 \t Numerical gradient: -0.781533\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 1)\n",
      "Your gradient: -1.173802 \t Numerical gradient: -1.173763\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 2)\n",
      "Your gradient: 0.665843 \t Numerical gradient: 0.665782\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 1)\n",
      "Your gradient: -0.905956 \t Numerical gradient: -0.905911\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 2)\n",
      "Your gradient: -1.205984 \t Numerical gradient: -1.206073\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 1)\n",
      "Your gradient: 0.389329 \t Numerical gradient: 0.389350\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 2)\n",
      "Your gradient: -0.395361 \t Numerical gradient: -0.395406\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 0)\n",
      "Your gradient: -3.021108 \t Numerical gradient: -3.020440\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 1)\n",
      "Your gradient: 0.483846 \t Numerical gradient: 0.484487\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 2)\n",
      "Your gradient: 1.281105 \t Numerical gradient: 1.281804\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 0)\n",
      "Your gradient: -1.554397 \t Numerical gradient: -1.553970\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 1)\n",
      "Your gradient: -0.115950 \t Numerical gradient: -0.115517\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 2)\n",
      "Your gradient: -0.709510 \t Numerical gradient: -0.709023\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 0)\n",
      "Your gradient: -3.108076 \t Numerical gradient: -3.107396\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 1)\n",
      "Your gradient: 0.315813 \t Numerical gradient: 0.316381\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 2)\n",
      "Your gradient: 0.214429 \t Numerical gradient: 0.215018\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 0)\n",
      "Your gradient: -1.696898 \t Numerical gradient: -1.696178\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 1)\n",
      "Your gradient: -0.681593 \t Numerical gradient: -0.681053\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 2)\n",
      "Your gradient: -0.789423 \t Numerical gradient: -0.788879\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 0)\n",
      "Your gradient: -2.615645 \t Numerical gradient: -2.614907\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 1)\n",
      "Your gradient: -0.225701 \t Numerical gradient: -0.225082\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 2)\n",
      "Your gradient: 0.414162 \t Numerical gradient: 0.414818\n",
      "Gradient check found: 25 erors!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 2)\n",
      "Your gradient: 0.440374 \t Numerical gradient: 0.440359\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 1)\n",
      "Your gradient: -0.211611 \t Numerical gradient: -0.211582\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 2)\n",
      "Your gradient: 0.260103 \t Numerical gradient: 0.260088\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 1)\n",
      "Your gradient: -0.280779 \t Numerical gradient: -0.280734\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 2)\n",
      "Your gradient: 0.640442 \t Numerical gradient: 0.640463\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 1)\n",
      "Your gradient: -0.335270 \t Numerical gradient: -0.335190\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 2)\n",
      "Your gradient: 0.213533 \t Numerical gradient: 0.213551\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 1)\n",
      "Your gradient: 0.156457 \t Numerical gradient: 0.156541\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 2)\n",
      "Your gradient: 0.284729 \t Numerical gradient: 0.284741\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 0)\n",
      "Your gradient: -0.467031 \t Numerical gradient: -0.467059\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 1)\n",
      "Your gradient: 0.112870 \t Numerical gradient: 0.113003\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 2)\n",
      "Your gradient: 0.330017 \t Numerical gradient: 0.330197\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 0)\n",
      "Your gradient: 0.167659 \t Numerical gradient: 0.167596\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 1)\n",
      "Your gradient: 0.027210 \t Numerical gradient: 0.027178\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 2)\n",
      "Your gradient: -0.015885 \t Numerical gradient: -0.015934\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 0)\n",
      "Your gradient: 0.190289 \t Numerical gradient: 0.190190\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 1)\n",
      "Your gradient: 0.115884 \t Numerical gradient: 0.115718\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 2)\n",
      "Your gradient: -0.108004 \t Numerical gradient: -0.108153\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 0)\n",
      "Your gradient: 0.218609 \t Numerical gradient: 0.218658\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 1)\n",
      "Your gradient: -0.244981 \t Numerical gradient: -0.244967\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 2)\n",
      "Your gradient: -0.296625 \t Numerical gradient: -0.296588\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 0)\n",
      "Your gradient: -0.109526 \t Numerical gradient: -0.109473\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 1)\n",
      "Your gradient: -0.010982 \t Numerical gradient: -0.011000\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 2)\n",
      "Your gradient: 0.090498 \t Numerical gradient: 0.090430\n",
      "Gradient check found: 24 erors!\n",
      "Gradient check failed.\n",
      "First gradient error found at index (0, 1)\n",
      "Your gradient: -0.933834 \t Numerical gradient: -0.933792\n",
      "Gradient check failed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First gradient error found at index (0, 2)\n",
      "Your gradient: -0.028432 \t Numerical gradient: -0.028518\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 0)\n",
      "Your gradient: -3.498695 \t Numerical gradient: -3.498596\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 1)\n",
      "Your gradient: -1.232545 \t Numerical gradient: -1.232354\n",
      "Gradient check failed.\n",
      "First gradient error found at index (1, 2)\n",
      "Your gradient: -0.913349 \t Numerical gradient: -0.913487\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 0)\n",
      "Your gradient: -3.544427 \t Numerical gradient: -3.544256\n",
      "Gradient check failed.\n",
      "First gradient error found at index (2, 1)\n",
      "Your gradient: -1.112075 \t Numerical gradient: -1.111818\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 0)\n",
      "Your gradient: -3.639383 \t Numerical gradient: -3.639114\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 1)\n",
      "Your gradient: -1.313386 \t Numerical gradient: -1.312985\n",
      "Gradient check failed.\n",
      "First gradient error found at index (3, 2)\n",
      "Your gradient: -0.573985 \t Numerical gradient: -0.573960\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 0)\n",
      "Your gradient: -3.039054 \t Numerical gradient: -3.038744\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 1)\n",
      "Your gradient: -0.473877 \t Numerical gradient: -0.473310\n",
      "Gradient check failed.\n",
      "First gradient error found at index (4, 2)\n",
      "Your gradient: -0.270831 \t Numerical gradient: -0.270842\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 0)\n",
      "Your gradient: -3.345431 \t Numerical gradient: -3.344589\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 1)\n",
      "Your gradient: 0.451597 \t Numerical gradient: 0.452434\n",
      "Gradient check failed.\n",
      "First gradient error found at index (5, 2)\n",
      "Your gradient: 0.970958 \t Numerical gradient: 0.971815\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 0)\n",
      "Your gradient: -0.996233 \t Numerical gradient: -0.996020\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 1)\n",
      "Your gradient: 0.056397 \t Numerical gradient: 0.056746\n",
      "Gradient check failed.\n",
      "First gradient error found at index (6, 2)\n",
      "Your gradient: -0.107919 \t Numerical gradient: -0.107520\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 0)\n",
      "Your gradient: -3.600278 \t Numerical gradient: -3.599101\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 1)\n",
      "Your gradient: -0.032643 \t Numerical gradient: -0.031608\n",
      "Gradient check failed.\n",
      "First gradient error found at index (7, 2)\n",
      "Your gradient: 0.033937 \t Numerical gradient: 0.034919\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 0)\n",
      "Your gradient: -1.275344 \t Numerical gradient: -1.274046\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 1)\n",
      "Your gradient: -0.367345 \t Numerical gradient: -0.366816\n",
      "Gradient check failed.\n",
      "First gradient error found at index (8, 2)\n",
      "Your gradient: -0.607937 \t Numerical gradient: -0.607377\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 0)\n",
      "Your gradient: -3.539069 \t Numerical gradient: -3.537632\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 1)\n",
      "Your gradient: -0.198409 \t Numerical gradient: -0.197452\n",
      "Gradient check failed.\n",
      "First gradient error found at index (9, 2)\n",
      "Your gradient: 0.215750 \t Numerical gradient: 0.216762\n",
      "Gradient check found: 28 erors!\n",
      "\n",
      "=== Results ===\n",
      "(11.166034533937173, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26946329, -1.36816163,  2.45190264],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.40998894,  0.18900944,  1.43377907],\n",
      "       [ 0.38141685, -0.17583739, -1.33385914],\n",
      "       [ 0.07000915, -0.03227499, -0.24483015],\n",
      "       [ 0.09466517, -0.04364169, -0.33105513],\n",
      "       [-0.13610223,  0.06274464,  0.47596535]]))\n",
      "(13.95909967928093, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.11784573, -1.66946271, -1.50150102],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.49788184,  0.22952904,  1.74115077],\n",
      "       [-0.02705063,  0.01247064,  0.09459919],\n",
      "       [-0.68194423,  0.31438385,  2.38483839],\n",
      "       [-0.84133286,  0.38786377,  2.94223899],\n",
      "       [-0.16100014,  0.07422285,  0.56303623]]))\n",
      "(0.79810723972277453, array([[ 0.23315912, -0.51577824, -0.8275274 ],\n",
      "       [ 0.11657956, -0.25788912, -0.4137637 ],\n",
      "       [ 0.11657956, -0.25788912, -0.4137637 ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80808133,  0.21858583, -0.54144425],\n",
      "       [-0.03557673, -0.0096235 ,  0.02383772],\n",
      "       [-0.12998916, -0.03516204,  0.08709753],\n",
      "       [-0.16461799, -0.04452913,  0.11030011],\n",
      "       [-0.47789745, -0.12927116,  0.32020889]]))\n",
      "(7.7551977658634321, array([[-3.23481416, -1.88429612, -2.68973263],\n",
      "       [-1.61740708, -0.94214806, -1.34486631],\n",
      "       [-1.61740708, -0.94214806, -1.34486631],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.2197535 ,  0.05944327, -0.14724293],\n",
      "       [-1.37692172, -0.37245703,  0.92258825],\n",
      "       [-1.55127717, -0.41962014,  1.03941282],\n",
      "       [-1.72238359, -0.46590439,  1.15406042],\n",
      "       [-2.36403144, -0.63947   ,  1.58398811]]))\n"
     ]
    }
   ],
   "source": [
    "test_normalize_rows()\n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 5000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\"\n",
    "    A helper function that loads previously saved parameters and resets\n",
    "    iteration start.\n",
    "    \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "\n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "\n",
    "\n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing=None, useSaved=False,\n",
    "        PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent\n",
    "\n",
    "    Implement the stochastic gradient descent method in this function.\n",
    "\n",
    "    Arguments:\n",
    "    f -- the function to optimize, it should take a single\n",
    "         argument and yield two outputs, a cost and the gradient\n",
    "         with respect to the arguments\n",
    "    x0 -- the initial point to start SGD from\n",
    "    step -- the step size for SGD\n",
    "    iterations -- total iterations to run SGD for\n",
    "    postprocessing -- postprocessing function for the parameters\n",
    "                      if necessary. In the case of word2vec we will need to\n",
    "                      normalize the word vectors to have unit length.\n",
    "    PRINT_EVERY -- specifies how many iterations to output loss\n",
    "\n",
    "    Return:\n",
    "    x -- the parameter value after SGD finishes\n",
    "    \"\"\"\n",
    "\n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "\n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "\n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "\n",
    "    x = x0\n",
    "\n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "\n",
    "    expcost = None\n",
    "\n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        # Don't forget to apply the postprocessing after every iteration!\n",
    "        # You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "        #raise NotImplementedError\n",
    "        \n",
    "        cost, grad = f(x)\n",
    "        x = x - step*grad\n",
    "        if postprocessing is not None:\n",
    "            x = postprocessing(x)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print \"iter %d: %f\" % (iter, expcost)\n",
    "\n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "\n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.41483678608e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.52445103582e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sgd_sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 1 result:\", t1\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 2 result:\", t2\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 3 result:\", t3\n",
    "    assert abs(t3) <= 1e-6\n",
    "\n",
    "    print \"\"\n",
    "sgd_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from utils.treebank import StanfordSentiment\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#from q3_word2vec import *\n",
    "#from q3_sgd import *\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "\n",
    "startTime=time.time()\n",
    "wordVectors = np.concatenate(\n",
    "    ((np.random.rand(nWords, dimVectors) - 0.5) /\n",
    "       dimVectors, np.zeros((nWords, dimVectors))),\n",
    "    axis=0)\n",
    "wordVectors = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C,\n",
    "        negSamplingCostAndGradient),\n",
    "    wordVectors, 0.3, 40000, None, True, PRINT_EVERY=10)\n",
    "# Note that normalization is not called here. This is not a bug,\n",
    "# normalizing during training loses the notion of length.\n",
    "\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "print \"training took %d seconds\" % (time.time() - startTime)\n",
    "\n",
    "# concatenate the input and output word vectors\n",
    "wordVectors = np.concatenate(\n",
    "    (wordVectors[:nWords,:], wordVectors[nWords:,:]),\n",
    "    axis=0)\n",
    "# wordVectors = wordVectors[:nWords,:] + wordVectors[nWords:,:]\n",
    "\n",
    "visualizeWords = [\n",
    "    \"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\",\n",
    "    \"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "    \"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\",\n",
    "    \"annoying\"]\n",
    "\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2])\n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i],\n",
    "        bbox=dict(facecolor='green', alpha=0.1))\n",
    "\n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))\n",
    "\n",
    "plt.savefig('q3_word_vectors.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
