{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 Exercise\n",
    "## CS224n NLP and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q1_softmax.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.general_utils import test_all_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_5:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0)\n",
    "print node1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print sess.run(node1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Compute the softmax function in tensorflow.\n",
    "\n",
    "    You might find the tensorflow functions tf.exp, tf.reduce_max,\n",
    "    tf.reduce_sum, tf.expand_dims useful. (Many solutions are possible, so you may\n",
    "    not need to use all of these functions). Recall also that many common\n",
    "    tensorflow operations are sugared (e.g. x * y does a tensor multiplication\n",
    "    if x and y are both tensors). Make sure to implement the numerical stability\n",
    "    fixes as in the previous homework!\n",
    "\n",
    "    Args:\n",
    "        x:   tf.Tensor with shape (n_samples, n_features). Note feature vectors are\n",
    "                  represented by row-vectors. (For simplicity, no need to handle 1-d\n",
    "                  input as in the previous homework)\n",
    "    Returns:\n",
    "        out: tf.Tensor with shape (n_sample, n_features). You need to construct this\n",
    "                  tensor in this problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \"\"\"\n",
    "    maxs = tf.reduce_max(x, axis = 1)\n",
    "    x_stab = tf.subtract(x, tf.expand_dims(maxs, 1))\n",
    "    x_exp = tf.exp(x_stab)\n",
    "    x_row_sum = tf.reduce_sum(x_exp, axis = 1)\n",
    "    \n",
    "    out = tf.divide(x_exp, x_row_sum)\n",
    "    \"\"\"\n",
    "    \n",
    "    exp_x = tf.exp(x - tf.reduce_max(x, reduction_indices=[1], keep_dims=True))\n",
    "    denominator = tf.reduce_sum(exp_x, reduction_indices=[1], keep_dims=True)\n",
    "    out = exp_x / denominator \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def cross_entropy_loss(y, yhat):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss in tensorflow.\n",
    "    The loss should be summed over the current minibatch.\n",
    "\n",
    "    y is a one-hot tensor of shape (n_samples, n_classes) and yhat is a tensor\n",
    "    of shape (n_samples, n_classes). y should be of dtype tf.int32, and yhat should\n",
    "    be of dtype tf.float32.\n",
    "\n",
    "    The functions tf.to_float, tf.reduce_sum, and tf.log might prove useful. (Many\n",
    "    solutions are possible, so you may not need to use all of these functions).\n",
    "\n",
    "    Note: You are NOT allowed to use the tensorflow built-in cross-entropy\n",
    "                functions.\n",
    "\n",
    "    Args:\n",
    "        y:    tf.Tensor with shape (n_samples, n_classes). One-hot encoded.\n",
    "        yhat: tf.Tensorwith shape (n_sample, n_classes). Each row encodes a\n",
    "                    probability distribution and should sum to 1.\n",
    "    Returns:\n",
    "        out:  tf.Tensor with shape (1,) (Scalar output). You need to construct this\n",
    "                    tensor in the problem.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \"\"\"\n",
    "    yhat_log = tf.log(yhat)\n",
    "    costs = -1 * tf.reduce_sum(tf.multiply(tf.to_float(y), yhat_log), axis = 1)\n",
    "    out = tf.reduce_sum(costs, axis = 0)\n",
    "    \"\"\"\n",
    "    logyhat = tf.log(yhat)\n",
    "    yfloat = tf.to_float(y)\n",
    "    out = - tf.reduce_sum(tf.multiply(yfloat, logyhat))\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax test 1 passed!\n",
      "Softmax test 2 passed!\n",
      "Basic (non-exhaustive) softmax tests pass\n",
      "\n",
      "Cross-entropy test 1 passed!\n",
      "Basic (non-exhaustive) cross-entropy tests pass\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of softmax to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "\n",
    "    test1 = softmax(tf.constant(np.array([[1001, 1002], [3, 4]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test1 = sess.run(test1)\n",
    "    test_all_close(\"Softmax test 1\", test1, np.array([[0.26894142,  0.73105858],\n",
    "                                                      [0.26894142,  0.73105858]]))\n",
    "\n",
    "    test2 = softmax(tf.constant(np.array([[-1001, -1002]]), dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "            test2 = sess.run(test2)\n",
    "    test_all_close(\"Softmax test 2\", test2, np.array([[0.73105858, 0.26894142]]))\n",
    "\n",
    "    print \"Basic (non-exhaustive) softmax tests pass\\n\"\n",
    "\n",
    "\n",
    "def test_cross_entropy_loss_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests of cross_entropy_loss to get you started.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    y = np.array([[0, 1], [1, 0], [1, 0]])\n",
    "    yhat = np.array([[.5, .5], [.5, .5], [.5, .5]])\n",
    "\n",
    "    test1 = cross_entropy_loss(\n",
    "            tf.constant(y, dtype=tf.int32),\n",
    "            tf.constant(yhat, dtype=tf.float32))\n",
    "    with tf.Session() as sess:\n",
    "        test1 = sess.run(test1)\n",
    "    expected = -3 * np.log(.5)\n",
    "    test_all_close(\"Cross-entropy test 1\", test1, expected)\n",
    "\n",
    "    print \"Basic (non-exhaustive) cross-entropy tests pass\"\n",
    "\n",
    "test_softmax_basic()\n",
    "test_cross_entropy_loss_basic()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### q1_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"Abstracts a Tensorflow graph for a learning task.\n",
    "\n",
    "    We use various Model classes as usual abstractions to encapsulate tensorflow\n",
    "    computational graphs. Each algorithm you will construct in this homework will\n",
    "    inherit from a Model object.\n",
    "    \"\"\"\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Adds placeholder variables to tensorflow computational graph.\n",
    "\n",
    "        Tensorflow uses placeholder variables to represent locations in a\n",
    "        computational graph where data is inserted.  These placeholders are used as\n",
    "        inputs by the rest of the model building and will be fed data during\n",
    "        training.\n",
    "\n",
    "        See for more information:\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/\n",
    "        io_ops.html#placeholders\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for one step of training.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If labels_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be a subset of the placeholder\n",
    "                    tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Implements the core of the model that transforms a batch \n",
    "        of input data into predictions.\n",
    "\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds Ops for the loss function to the computational graph.\n",
    "\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar) output\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        sess.run() to train the model. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor (a scalar).\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Each Model must re-implement this method.\")\n",
    "\n",
    "    def train_on_batch(self, sess, inputs_batch, labels_batch):\n",
    "        \"\"\"Perform one step of gradient descent on the provided batch of data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "            labels_batch: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            loss: loss over the batch (a scalar)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch, labels_batch=labels_batch)\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed)\n",
    "        return loss\n",
    "\n",
    "    def predict_on_batch(self, sess, inputs_batch):\n",
    "        \"\"\"Make predictions for the provided batch of data\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            input_batch: np.ndarray of shape (n_samples, n_features)\n",
    "        Returns:\n",
    "            predictions: np.ndarray of shape (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        feed = self.create_feed_dict(inputs_batch)\n",
    "        predictions = sess.run(self.pred, feed_dict=feed)\n",
    "        return predictions\n",
    "\n",
    "    def build(self):\n",
    "        self.add_placeholders()\n",
    "        self.pred = self.add_prediction_op()\n",
    "        self.loss = self.add_loss_op(self.pred)\n",
    "        self.train_op = self.add_training_op(self.loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### q1_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    \"\"\"Holds model hyperparams and data information.\n",
    "\n",
    "    The config class is used to store various hyperparameters and dataset\n",
    "    information parameters. Model objects are passed a Config() object at\n",
    "    instantiation.\n",
    "    \"\"\"\n",
    "    n_samples = 1024\n",
    "    n_features = 100\n",
    "    n_classes = 5\n",
    "    batch_size = 64\n",
    "    n_epochs = 50\n",
    "    lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.general_utils import get_minibatches\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxModel(Model):\n",
    "    \"\"\"Implements a Softmax classifier with cross-entropy loss.\"\"\"\n",
    "\n",
    "    def add_placeholders(self):\n",
    "        \"\"\"Generates placeholder variables to represent the input tensors.\n",
    "\n",
    "        These placeholders are used as inputs by the rest of the model building\n",
    "        and will be fed data during training.\n",
    "\n",
    "        Adds following nodes to the computational graph\n",
    "\n",
    "        input_placeholder: Input placeholder tensor of shape\n",
    "                                              (batch_size, n_features), type tf.float32\n",
    "        labels_placeholder: Labels placeholder tensor of shape\n",
    "                                              (batch_size, n_classes), type tf.int32\n",
    "\n",
    "        Add these placeholders to self as the instance variables\n",
    "            self.input_placeholder\n",
    "            self.labels_placeholder\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        self.input_placeholder = tf.placeholder(tf.float32, \\\n",
    "                                                shape=[self.config.batch_size, \\\n",
    "                                                       self.config.n_features])\n",
    "        self.labels_placeholder = tf.placeholder(tf.int32, \\\n",
    "                                                 shape=[self.config.batch_size, \\\n",
    "                                                        self.config.n_classes])\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "\n",
    "    def create_feed_dict(self, inputs_batch, labels_batch=None):\n",
    "        \"\"\"Creates the feed_dict for training the given step.\n",
    "\n",
    "        A feed_dict takes the form of:\n",
    "        feed_dict = {\n",
    "                <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "                ....\n",
    "        }\n",
    "\n",
    "        If label_batch is None, then no labels are added to feed_dict.\n",
    "\n",
    "        Hint: The keys for the feed_dict should be the placeholder\n",
    "                tensors created in add_placeholders.\n",
    "\n",
    "        Args:\n",
    "            inputs_batch: A batch of input data.\n",
    "            labels_batch: A batch of label data.\n",
    "        Returns:\n",
    "            feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        feed_dict = {self.input_placeholder : inputs_batch}\n",
    "        if labels_batch is not None:\n",
    "            feed_dict[self.labels_placeholder] = labels_batch\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return feed_dict\n",
    "\n",
    "    def add_prediction_op(self):\n",
    "        \"\"\"Adds the core transformation for this model which transforms a batch of input\n",
    "        data into a batch of predictions. In this case, the transformation is a \n",
    "        linear layer plus a\n",
    "        softmax transformation:\n",
    "\n",
    "        y = softmax(Wx + b)\n",
    "\n",
    "        Hint: Make sure to create tf.Variables as needed.\n",
    "        Hint: For this simple use-case, it's sufficient to initialize both weights W\n",
    "                    and biases b with zeros.\n",
    "\n",
    "        Args:\n",
    "            input_data: A tensor of shape (batch_size, n_features).\n",
    "        Returns:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        W = tf.Variable(tf.zeros([self.config.n_features, self.config.n_classes], \\\n",
    "                                 dtype=tf.float32))\n",
    "        b = tf.Variable(tf.zeros([self.config.n_classes], dtype=tf.float32))\n",
    "        pred = softmax(tf.matmul(self.input_placeholder, W) + b)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return pred\n",
    "\n",
    "    def add_loss_op(self, pred):\n",
    "        \"\"\"Adds cross_entropy_loss ops to the computational graph.\n",
    "\n",
    "        Hint: Use the cross_entropy_loss function we defined. This should be a very\n",
    "                    short function.\n",
    "        Args:\n",
    "            pred: A tensor of shape (batch_size, n_classes)\n",
    "        Returns:\n",
    "            loss: A 0-d tensor (scalar)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        loss = cross_entropy_loss(self.labels_placeholder, pred)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return loss\n",
    "\n",
    "    def add_training_op(self, loss):\n",
    "        \"\"\"Sets up the training Ops.\n",
    "\n",
    "        Creates an optimizer and applies the gradients to all trainable variables.\n",
    "        The Op returned by this function is what must be passed to the\n",
    "        `sess.run()` call to cause the model to train. See\n",
    "\n",
    "        https://www.tensorflow.org/versions/r0.7/api_docs/python/train.html#Optimizer\n",
    "\n",
    "        for more information.\n",
    "\n",
    "        Hint: Use tf.train.GradientDescentOptimizer to get an optimizer object.\n",
    "                    Calling optimizer.minimize() will return a train_op object.\n",
    "\n",
    "        Args:\n",
    "            loss: Loss tensor, from cross_entropy_loss.\n",
    "        Returns:\n",
    "            train_op: The Op for training.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        \n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.config.lr)\n",
    "        train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        return train_op\n",
    "\n",
    "    def run_epoch(self, sess, inputs, labels):\n",
    "        \"\"\"Runs an epoch of training.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session() object\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            average_loss: scalar. Average minibatch loss of model on epoch.\n",
    "        \"\"\"\n",
    "        n_minibatches, total_loss = 0, 0\n",
    "        for input_batch, labels_batch in get_minibatches([inputs, labels], \\\n",
    "                                                         self.config.batch_size):\n",
    "            n_minibatches += 1\n",
    "            total_loss += self.train_on_batch(sess, input_batch, labels_batch)\n",
    "        return total_loss / n_minibatches\n",
    "\n",
    "    def fit(self, sess, inputs, labels):\n",
    "        \"\"\"Fit model on provided data.\n",
    "\n",
    "        Args:\n",
    "            sess: tf.Session()\n",
    "            inputs: np.ndarray of shape (n_samples, n_features)\n",
    "            labels: np.ndarray of shape (n_samples, n_classes)\n",
    "        Returns:\n",
    "            losses: list of loss per epoch\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        for epoch in range(self.config.n_epochs):\n",
    "            start_time = time.time()\n",
    "            average_loss = self.run_epoch(sess, inputs, labels)\n",
    "            duration = time.time() - start_time\n",
    "            print 'Epoch {:}: loss = {:.2f} ({:.3f} sec)'.format(epoch, \\\n",
    "                                                                 average_loss, duration)\n",
    "            losses.append(average_loss)\n",
    "        return losses\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"Initializes the model.\n",
    "\n",
    "        Args:\n",
    "            config: A model configuration object of type Config\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_softmax_model():\n",
    "    \"\"\"Train softmax model for a number of steps.\"\"\"\n",
    "    config = Config()\n",
    "\n",
    "    # Generate random data to train the model on\n",
    "    np.random.seed(1234)\n",
    "    inputs = np.random.rand(config.n_samples, config.n_features)\n",
    "    labels = np.zeros((config.n_samples, config.n_classes), dtype=np.int32)\n",
    "    labels[:, 0] = 1\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    # (not required but good practice)\n",
    "    with tf.Graph().as_default():\n",
    "        # Build the model and add the variable initializer Op\n",
    "        model = SoftmaxModel(config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        # If you are using an old version of TensorFlow, you may have to use\n",
    "        # this initializer instead.\n",
    "        # init = tf.initialize_all_variables()\n",
    "\n",
    "        # Create a session for running Ops in the Graph\n",
    "        with tf.Session() as sess:\n",
    "            # Run the Op to initialize the variables.\n",
    "            sess.run(init)\n",
    "            # Fit the model\n",
    "            losses = model.fit(sess, inputs, labels)\n",
    "\n",
    "    # If Ops are implemented correctly, the average loss should fall close to zero\n",
    "    # rapidly.\n",
    "    assert losses[-1] < .5\n",
    "    print \"Basic (non-exhaustive) classifier tests pass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 59.18 (0.019 sec)\n",
      "Epoch 1: loss = 20.32 (0.009 sec)\n",
      "Epoch 2: loss = 10.92 (0.011 sec)\n",
      "Epoch 3: loss = 7.30 (0.009 sec)\n",
      "Epoch 4: loss = 5.44 (0.009 sec)\n",
      "Epoch 5: loss = 4.32 (0.009 sec)\n",
      "Epoch 6: loss = 3.58 (0.009 sec)\n",
      "Epoch 7: loss = 3.05 (0.009 sec)\n",
      "Epoch 8: loss = 2.65 (0.009 sec)\n",
      "Epoch 9: loss = 2.35 (0.009 sec)\n",
      "Epoch 10: loss = 2.11 (0.009 sec)\n",
      "Epoch 11: loss = 1.91 (0.009 sec)\n",
      "Epoch 12: loss = 1.75 (0.009 sec)\n",
      "Epoch 13: loss = 1.61 (0.009 sec)\n",
      "Epoch 14: loss = 1.49 (0.009 sec)\n",
      "Epoch 15: loss = 1.39 (0.009 sec)\n",
      "Epoch 16: loss = 1.30 (0.009 sec)\n",
      "Epoch 17: loss = 1.22 (0.009 sec)\n",
      "Epoch 18: loss = 1.15 (0.009 sec)\n",
      "Epoch 19: loss = 1.09 (0.009 sec)\n",
      "Epoch 20: loss = 1.03 (0.009 sec)\n",
      "Epoch 21: loss = 0.98 (0.009 sec)\n",
      "Epoch 22: loss = 0.94 (0.010 sec)\n",
      "Epoch 23: loss = 0.89 (0.013 sec)\n",
      "Epoch 24: loss = 0.86 (0.012 sec)\n",
      "Epoch 25: loss = 0.82 (0.010 sec)\n",
      "Epoch 26: loss = 0.79 (0.011 sec)\n",
      "Epoch 27: loss = 0.76 (0.010 sec)\n",
      "Epoch 28: loss = 0.73 (0.009 sec)\n",
      "Epoch 29: loss = 0.71 (0.009 sec)\n",
      "Epoch 30: loss = 0.68 (0.009 sec)\n",
      "Epoch 31: loss = 0.66 (0.009 sec)\n",
      "Epoch 32: loss = 0.64 (0.009 sec)\n",
      "Epoch 33: loss = 0.62 (0.009 sec)\n",
      "Epoch 34: loss = 0.60 (0.009 sec)\n",
      "Epoch 35: loss = 0.58 (0.009 sec)\n",
      "Epoch 36: loss = 0.57 (0.009 sec)\n",
      "Epoch 37: loss = 0.55 (0.009 sec)\n",
      "Epoch 38: loss = 0.54 (0.010 sec)\n",
      "Epoch 39: loss = 0.52 (0.010 sec)\n",
      "Epoch 40: loss = 0.51 (0.010 sec)\n",
      "Epoch 41: loss = 0.50 (0.010 sec)\n",
      "Epoch 42: loss = 0.48 (0.010 sec)\n",
      "Epoch 43: loss = 0.47 (0.013 sec)\n",
      "Epoch 44: loss = 0.46 (0.012 sec)\n",
      "Epoch 45: loss = 0.45 (0.011 sec)\n",
      "Epoch 46: loss = 0.44 (0.012 sec)\n",
      "Epoch 47: loss = 0.43 (0.010 sec)\n",
      "Epoch 48: loss = 0.42 (0.010 sec)\n",
      "Epoch 49: loss = 0.41 (0.009 sec)\n",
      "Basic (non-exhaustive) classifier tests pass\n"
     ]
    }
   ],
   "source": [
    "test_softmax_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
